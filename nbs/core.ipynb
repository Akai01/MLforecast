{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import concurrent.futures\n",
    "import copy\n",
    "import inspect\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from fastcore.foundation import patch, tuplify\n",
    "from numba import njit\n",
    "from window_ops.shift import shift_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import *\n",
    "from window_ops.rolling import *\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required input format is a dataframe with an index named `unique_id` with an unique identifier for each time serie, a column `ds` with the datestamp and a column `y` with the values of the serie. Every other column is considered a static feature unless stated otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(20, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we'll just take one time serie here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = series.index.unique(level='unique_id')\n",
    "serie = series.loc[[uids[0]]]\n",
    "serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "date_features_dtypes = {\n",
    "    'year': np.uint16,\n",
    "    'month': np.uint8,\n",
    "    'day': np.uint8,\n",
    "    'hour': np.uint8,\n",
    "    'minute': np.uint8,\n",
    "    'second': np.uint8,\n",
    "    'dayofyear': np.uint16,\n",
    "    'day_of_year': np.uint16,\n",
    "    'weekofyear': np.uint8,\n",
    "    'week': np.uint8,\n",
    "    'dayofweek': np.uint8,\n",
    "    'day_of_week': np.uint8,\n",
    "    'weekday': np.uint8,\n",
    "    'quarter': np.uint8,\n",
    "    'daysinmonth': np.uint8,\n",
    "    'is_month_start': np.uint8,\n",
    "    'is_month_end': np.uint8,\n",
    "    'is_quarter_start': np.uint8,\n",
    "    'is_quarter_end': np.uint8,\n",
    "    'is_year_start': np.uint8,\n",
    "    'is_year_end': np.uint8,\n",
    "}\n",
    "\n",
    "\n",
    "@njit\n",
    "def _append_new(data, indptr, new):\n",
    "    \"\"\"Append each value of new to each group in data formed by indptr.\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    new_data = np.empty(data.size + new.size, dtype=data.dtype)\n",
    "    new_indptr = indptr.copy()\n",
    "    new_indptr[1:] += np.arange(1, n_series + 1)\n",
    "    for i in range(n_series):\n",
    "        new_data[new_indptr[i] : new_indptr[i+1] - 1] = data[indptr[i] : indptr[i + 1]]\n",
    "        new_data[new_indptr[i+1] - 1] = new[i]\n",
    "    return new_data, new_indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GroupedArray:\n",
    "    \"\"\"Array made up from different groups. Can be thought of as a list of arrays.\n",
    "    \n",
    "    All the data is stored in a single 1d array `data`.\n",
    "    The indices for the group boundaries are stored in another 1d array `indptr`.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, indptr: np.ndarray):\n",
    "        self.data = data\n",
    "        self.indptr = indptr\n",
    "        self.ngroups = len(indptr) - 1\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.ngroups\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> np.ndarray:\n",
    "        return self.data[self.indptr[idx]:self.indptr[idx+1]]\n",
    "        \n",
    "    def take_from_groups(self, idx: Union[int, slice]) -> 'GroupedArray':\n",
    "        \"\"\"Takes `idx` from each group in the array.\"\"\"\n",
    "        ranges = [range(self.indptr[i], self.indptr[i+1])[idx] for i in range(self.ngroups)]\n",
    "        items = [self.data[rng] for rng in ranges]\n",
    "        sizes = np.array([item.size for item in items])\n",
    "        data = np.hstack(items)\n",
    "        indptr = np.append(0, sizes.cumsum())\n",
    "        return GroupedArray(data, indptr)\n",
    "        \n",
    "    def append(self, new: np.ndarray) -> 'GroupedArray':\n",
    "        \"\"\"Appends each element of `new` to each existing group. Returns a copy.\"\"\"\n",
    "        if new.size != self.ngroups:\n",
    "            raise ValueError(f'new must be of size {self.ngroups}')\n",
    "        new_data, new_indptr = _append_new(self.data, self.indptr, new)\n",
    "        return GroupedArray(new_data, new_indptr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'GroupedArray(ndata={self.data.size}, ngroups={self.ngroups})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GroupedArray` is used internally for storing the series values and performing transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10, dtype=np.float32)\n",
    "indptr = np.array([0, 2, 10])  # group 1: [0, 1], group 2: [2..9]\n",
    "ga = GroupedArray(data, indptr)\n",
    "\n",
    "# take the last two observations from every group\n",
    "last_2 = ga.take_from_groups(slice(-2, None))\n",
    "np.testing.assert_equal(last_2.data, np.array([0, 1, 8, 9]))\n",
    "np.testing.assert_equal(last_2.indptr, np.array([0, 2, 4]))\n",
    "\n",
    "# take the last four observations from every group\n",
    "last_4 = ga.take_from_groups(slice(-4, None))\n",
    "np.testing.assert_equal(last_4.data, np.array([0, 1, 6, 7, 8, 9]))\n",
    "np.testing.assert_equal(last_4.indptr, np.array([0, 2, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "@njit\n",
    "def _identity(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Do nothing to the input.\"\"\"\n",
    "    return x\n",
    "\n",
    "\n",
    "@njit(nogil=True)\n",
    "def _transform_series(data, indptr, updates_only, lag, func, *args) -> np.ndarray:\n",
    "    \"\"\"Shifts every group in data by `lag` and computes `func(shifted, *args)` on it.\"\"\"\n",
    "    n_series = len(indptr) - 1\n",
    "    if updates_only:\n",
    "        out = np.empty_like(data[:n_series])\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i]:indptr[i+1]], lag)\n",
    "            out[i] = func(lagged, *args)[-1]        \n",
    "    else:\n",
    "        out = np.empty_like(data)\n",
    "        for i in range(n_series):\n",
    "            lagged = shift_array(data[indptr[i]:indptr[i+1]], lag)\n",
    "            out[indptr[i]:indptr[i+1]] = func(lagged, *args)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _build_transform_name(lag, tfm, *args) -> str:\n",
    "    \"\"\"Creates a name for a transformation based on `lag`, the name of the function and its arguments.\"\"\"\n",
    "    if lag == 0:\n",
    "        return f'lag-{args[0]}'\n",
    "    tfm_name = f'{tfm.__name__}_lag-{lag}'\n",
    "    func_params = list(inspect.signature(tfm).parameters.items())[1:]  # remove input array argument\n",
    "    changed_params = [f'{name}-{value}' for value, (name, param) in zip(args, func_params) if param.default != value]\n",
    "    if changed_params:\n",
    "        tfm_name += '_' + '_'.join(changed_params)\n",
    "    return tfm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeSeries:\n",
    "    \"\"\"Utility class for storing and transforming time series data.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 series_df: pd.DataFrame,\n",
    "                 freq: str = 'D',\n",
    "                 lags: List[int] = [],\n",
    "                 lag_transforms: Dict[int, List[Tuple]] = {},\n",
    "                 date_features: List[str] = [],\n",
    "                 static_features: Optional[List[str]] = None,\n",
    "                 num_threads: Optional[int] = None):\n",
    "        if not series_df.index.is_monotonic_increasing:\n",
    "            series_df = series_df.sort_index()\n",
    "        data = series_df.y.values\n",
    "        if data.dtype not in (np.float32, np.float64):\n",
    "            data = data.astype(np.float32)\n",
    "        sizes = series_df.groupby('unique_id').size().values\n",
    "        cumsizes = sizes.cumsum()\n",
    "        indptr = np.append(0, cumsizes)\n",
    "        self.ga = GroupedArray(data, indptr)\n",
    "        self.uids = series_df.index.unique(level='unique_id')\n",
    "        self.last_dates = series_df.index.get_level_values('ds')[cumsizes - 1]\n",
    "        self.freq = pd.tseries.frequencies.to_offset(freq)\n",
    "        self.static_features = series_df.iloc[cumsizes - 1].reset_index('ds', drop=True).drop(columns='y')\n",
    "        if static_features is not None:\n",
    "            self.static_features = self.static_features[static_features]\n",
    "        self.num_threads = num_threads or os.cpu_count()\n",
    "        self.date_features = date_features\n",
    "        \n",
    "        self.transforms: Dict[str, Tuple[Any, ...]] = OrderedDict()\n",
    "        for lag in lags:\n",
    "            self.transforms[f'lag-{lag}'] = (lag, _identity)\n",
    "        for lag in lag_transforms.keys():\n",
    "            for tfm_args in lag_transforms[lag]:\n",
    "                tfm, *args = tuplify(tfm_args)\n",
    "                tfm_name = _build_transform_name(lag, tfm, *args)\n",
    "                self.transforms[tfm_name] = (lag, tfm, *args)\n",
    "                \n",
    "        self.y_pred: List[np.ndarray] = []\n",
    "        self.curr_dates = self.last_dates\n",
    "        self.test_dates: List[pd.DatetimeIndex] = []\n",
    "\n",
    "    @property\n",
    "    def n_series(self):\n",
    "        return self.ga.ngroups\n",
    "                \n",
    "    @property\n",
    "    def features(self):\n",
    "        return list(self.transforms.keys()) + self.date_features\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return f'TimeSeries(n_series={self.n_series}, freq={self.freq}, transforms={self.transforms.keys()}, date_features={self.date_features})'\n",
    "    \n",
    "    def _apply_transforms(self, updates_only: bool = False):\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0\n",
    "        for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "            results[tfm_name] =  _transform_series(self.ga.data, self.ga.indptr, updates_only, lag - offset, tfm, *args)\n",
    "        return results\n",
    "\n",
    "    def _apply_multithreaded_transforms(self, updates_only: bool = False):\n",
    "        future_to_result = {}\n",
    "        results = {}\n",
    "        offset = 1 if updates_only else 0        \n",
    "        with concurrent.futures.ThreadPoolExecutor(self.num_threads) as executor:\n",
    "            for tfm_name, (lag, tfm, *args) in self.transforms.items():\n",
    "                future = executor.submit(_transform_series, self.ga.data, self.ga.indptr, updates_only, lag - offset, tfm, *args)\n",
    "                future_to_result[future] = tfm_name\n",
    "            for future in concurrent.futures.as_completed(future_to_result):\n",
    "                tfm_name = future_to_result[future]\n",
    "                results[tfm_name] = future.result()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class has to extract each series values and store it in a contiguous numpy 1d-array. In order to achieve this, the input dataframe must be sorted by `unique_id` and `ds`. To achieve this we set both as indices and make sure they're sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = serie.set_index('ds', append=True)\n",
    "serie.index.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TimeSeries` class takes care of defining the transformations to be performed (`lags`, `lag_transforms` and `date_features`) as well as storing the necessary data to update them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [7]\n",
    "lag_transforms = {\n",
    "    1: [\n",
    "        expanding_mean, \n",
    "        (rolling_mean, 7)\n",
    "    ]\n",
    "}\n",
    "date_features = ['dayofweek']\n",
    "\n",
    "ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, date_features=date_features)\n",
    "\n",
    "test_eq(ts.uids, ['id_00'])\n",
    "test_eq(ts.last_dates, [serie.index.get_level_values('ds').max()])\n",
    "test_eq(ts.date_features, date_features)\n",
    "test_eq(ts.transforms, {'lag-7': (7, _identity), \n",
    "                        'expanding_mean_lag-1': (1, expanding_mean), \n",
    "                        'rolling_mean_lag-1_window_size-7': (1, rolling_mean, 7)})\n",
    "test_eq(ts.static_features, serie.tail(1).reset_index('ds', drop=True).drop(columns='y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def compute_transforms(self: TimeSeries) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute the transformations defined in the constructor.\n",
    "    \n",
    "    If `num_threads > 1` these are computed using multithreading.\"\"\"\n",
    "    if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "        return self._apply_transforms()\n",
    "    return self._apply_multithreaded_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we instantiate a `TimeSeries` class we can call `compute_transforms` to get the values of all the transformations. These are returned in a dictionary where the keys are the name that was assigned the each transformation and the values are the result of the transformation applied to the time serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = serie.y.values\n",
    "lag_1 = shift_array(y, 1)\n",
    "\n",
    "for num_threads in (1, 2):\n",
    "    ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, num_threads=num_threads)\n",
    "    transforms = ts.compute_transforms()\n",
    "\n",
    "    np.testing.assert_equal(transforms['lag-7'], shift_array(y, 7))\n",
    "    np.testing.assert_equal(transforms['expanding_mean_lag-1'], expanding_mean(lag_1))\n",
    "    np.testing.assert_equal(transforms['rolling_mean_lag-1_window_size-7'], rolling_mean(lag_1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def update_y(self: TimeSeries, new: np.ndarray) -> None:\n",
    "    \"\"\"Updates the value of y for the predictions and for the features updates.\"\"\"\n",
    "    if len(self.y_pred) == 0:\n",
    "        self.y_pred = []\n",
    "    self.y_pred.append(new)\n",
    "    new_arr = np.asarray(new)\n",
    "    self.ga = self.ga.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(serie)\n",
    "max_size = np.diff(ts.ga.indptr)\n",
    "ts.update_y([1])\n",
    "ts.update_y([2])\n",
    "\n",
    "test_eq(np.diff(ts.ga.indptr), max_size + 2)\n",
    "test_eq(ts.ga.data[-1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def update_features(self: TimeSeries) -> pd.DataFrame:\n",
    "    \"\"\"Compute the current values of all the features using the latest values of the target.\"\"\"\n",
    "    if self.curr_dates.equals(self.last_dates):\n",
    "        self.curr_dates = self.last_dates.copy()\n",
    "        self.test_dates = []\n",
    "    self.curr_dates += self.freq\n",
    "    self.test_dates.append(self.curr_dates)\n",
    "    \n",
    "    if self.num_threads == 1 or len(self.transforms) == 1:\n",
    "        features = self._apply_transforms(updates_only=True)\n",
    "    else:\n",
    "        features = self._apply_multithreaded_transforms(updates_only=True)\n",
    "    \n",
    "    for feature in self.date_features:\n",
    "        feat_vals = getattr(self.curr_dates, feature).values\n",
    "        features[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "        \n",
    "    features_df = pd.DataFrame(features, columns=self.features, index=self.uids)\n",
    "    results_df = self.static_features.join(features_df)\n",
    "    results_df['ds'] = self.curr_dates\n",
    "    results_df = results_df.set_index('ds', append=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(serie, lags=lags, lag_transforms=lag_transforms, date_features=date_features)\n",
    "updates = ts.update_features()\n",
    "\n",
    "# these have an offset becase we can now \"see\" our last y value\n",
    "last_date = serie.index.get_level_values('ds').max()\n",
    "expected_idx = pd.MultiIndex.from_tuples([(ts.uids[0], last_date + ts.freq)], names=['unique_id', 'ds'])\n",
    "expected = pd.DataFrame({\n",
    "    'lag-7': shift_array(y, 6)[-1],\n",
    "    'expanding_mean_lag-1': expanding_mean(y)[-1],\n",
    "    'rolling_mean_lag-1_window_size-7': rolling_mean(y, 7)[-1],\n",
    "    'dayofweek': np.uint8([getattr(last_date + pd.tseries.offsets.Day(), 'dayofweek')])},\n",
    "    index=expected_idx\n",
    ")\n",
    "statics = serie.tail(1).drop('y', 1).reset_index('ds', drop=True)\n",
    "assert updates.equals(statics.join(expected))\n",
    "\n",
    "test_eq(ts.curr_dates, serie.index.get_level_values('ds')[[-1]] + pd.tseries.offsets.Day())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_predictions(self: TimeSeries) -> pd.DataFrame:\n",
    "    \"\"\"Get all the predicted values with their corresponding ids and datestamps.\"\"\"\n",
    "    n_preds = len(self.y_pred)\n",
    "    idx = pd.Index(chain.from_iterable([uid] * n_preds for uid in self.uids), name='unique_id')\n",
    "    df = pd.DataFrame({\n",
    "        'ds': np.array(self.test_dates).ravel('F'), \n",
    "        'y_pred': np.array(self.y_pred).ravel('F')},\n",
    "        index=idx)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def preprocessing_flow(df: pd.DataFrame,\n",
    "                       freq: str = 'D',\n",
    "                       lags: List[int] = [],\n",
    "                       lag_transforms: Dict[int, List[Tuple]] = {},\n",
    "                       date_features: List[str] = [],\n",
    "                       dropna: bool = True,\n",
    "                       keep_last_n: Optional[int] = None,\n",
    "                       num_threads: Optional[int] = os.cpu_count()) -> Tuple[TimeSeries, pd.DataFrame]:\n",
    "    \"\"\"Standard preprocessing flow.\"\"\"\n",
    "    df = df.set_index('ds', append=True).sort_index()\n",
    "    series = TimeSeries(df, freq, lags, lag_transforms, date_features, \n",
    "                        num_threads=num_threads)\n",
    "    df = df.reset_index('ds')\n",
    "    \n",
    "    features = series.compute_transforms()  # type: ignore\n",
    "    for k in series.transforms.keys():\n",
    "        df[k] = features[k]\n",
    "  \n",
    "    if dropna:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    for feature in date_features:\n",
    "        feat_vals = getattr(df.ds.dt, feature).values\n",
    "        df[feature] = feat_vals.astype(date_features_dtypes[feature])\n",
    "    \n",
    "    if keep_last_n is not None:\n",
    "        series.ga = series.ga.take_from_groups(slice(-keep_last_n, None))\n",
    "\n",
    "    return series, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        1: [\n",
    "            (expanding_mean,),\n",
    "            (expanding_std,),\n",
    "        ],\n",
    "        2: [\n",
    "            (rolling_mean, 7),\n",
    "            (rolling_mean, 14),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    keep_last_n=15,\n",
    "    num_threads=2\n",
    ")\n",
    "ts, df = preprocessing_flow(series, **config)\n",
    "\n",
    "expected_lags = ['lag-7', 'lag-14']\n",
    "expected_transforms = ['expanding_mean_lag-1', 'expanding_std_lag-1', 'rolling_mean_lag-2_window_size-7', 'rolling_mean_lag-2_window_size-14']\n",
    "expected_date_features = ['dayofweek', 'month', 'year']\n",
    "\n",
    "test_eq(ts.features, expected_lags + expected_transforms + expected_date_features)\n",
    "test_eq(ts.static_features.columns.tolist() + ts.features, df.columns.drop(['ds', 'y']).tolist())\n",
    "# we dropped 2 rows because of the lag 2 and 13 more to have the window of size 14\n",
    "test_eq(df.shape[0], series.shape[0] - (2 + 13) * ts.n_series)\n",
    "test_eq(ts.ga.data.size, ts.ga.ngroups * config['keep_last_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predictions_flow(series: TimeSeries,\n",
    "                     model,\n",
    "                     horizon: int) -> pd.DataFrame:\n",
    "    series = copy.copy(series)\n",
    "    for _ in range(horizon):\n",
    "        new_x = series.update_features()  # type: ignore\n",
    "        if isinstance(model, xgb.Booster):\n",
    "            new_x = xgb.DMatrix(new_x)\n",
    "        predictions = model.predict(new_x)\n",
    "        series.update_y(predictions)  # type: ignore\n",
    "    return series.get_predictions()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return X['lag-7'].values\n",
    "    \n",
    "horizon = 7\n",
    "model = DummyModel()\n",
    "predictions = predictions_flow(ts, model, horizon)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_series = series.groupby('unique_id')\n",
    "expected_preds = grouped_series['y'].tail(7)\n",
    "expected_dsmin = grouped_series['ds'].max() + ts.freq\n",
    "expected_dsmax = grouped_series['ds'].max() + horizon * ts.freq\n",
    "\n",
    "grouped_preds = predictions.groupby('unique_id')\n",
    "assert predictions['y_pred'].equals(expected_preds)\n",
    "assert grouped_preds['ds'].min().equals(expected_dsmin)\n",
    "assert grouped_preds['ds'].max().equals(expected_dsmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
