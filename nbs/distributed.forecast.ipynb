{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f1aea-1fd9-4818-8fe8-cd8655813129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp distributed.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d947-ce42-40ec-8b54-623ec2189dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82054e-db3d-43ca-a4fe-c397e351ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898fb6b-131b-4895-bb0e-5c763a9325af",
   "metadata": {},
   "source": [
    "# Distributed forecast\n",
    "\n",
    "> Distributed pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3690-1951-487a-8c66-ba1b2cc01756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, default_client\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from mlforecast.core import predictions_flow, preprocessing_flow\n",
    "from mlforecast.forecast import Forecast\n",
    "from mlforecast.distributed.core import distributed_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42449d-cf14-4e8e-b7fa-43f095851536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistributedForecast(Forecast):\n",
    "    \"\"\"Full pipeline encapsulation.\n",
    "    \n",
    "    Takes a model (`LGBMForecast` or `XGBForecast`), a flow configuration and a client.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, flow_config: Dict, client: Optional[Client] = None):\n",
    "        self.model = model\n",
    "        self.flow_config = flow_config\n",
    "        self.client = client or default_client()\n",
    "        self.model.client = self.client\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'DistributedForecast(model={self.model}, flow_config={self.flow_config})'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf694337-530b-4428-be4c-a78d08a9267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def preprocess(self: DistributedForecast,\n",
    "               data: dd.DataFrame,\n",
    "               prep_fn: Callable = preprocessing_flow) -> dd.DataFrame:\n",
    "    \"\"\"Applies `prep_fn(partition, **self.flow_config)` on each partition of `data`.\n",
    "    \n",
    "    Saves the resulting `TimeSeries` objects as well as the divisions in `data` for the forecasting step.\n",
    "    Returns a dask dataframe with the computed features.\"\"\"\n",
    "    self.data_divisions = data.divisions\n",
    "    self.ts, series_ddf = distributed_preprocess(data, self.flow_config, self.client, prep_fn)\n",
    "    return series_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232df804-1eb5-4e0f-84df-e28f7375b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def fit(self: DistributedForecast,\n",
    "        data: dd.DataFrame,\n",
    "        prep_fn: Callable = preprocessing_flow,\n",
    "        **fit_kwargs) -> DistributedForecast:\n",
    "    \"\"\"Perform the preprocessing and fit the model.\"\"\"\n",
    "    train_ddf = self.preprocess(data, prep_fn)\n",
    "    X, y = train_ddf.drop(columns=['ds', 'y']), train_ddf.y\n",
    "    self.model.fit(X, y, **fit_kwargs)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4341e-a226-46f8-842c-391629c2cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict(self: DistributedForecast,\n",
    "            horizon: int,\n",
    "            predict_fn: Callable = predictions_flow,\n",
    "            **predict_fn_kwargs) -> dd.DataFrame:\n",
    "    \"\"\"Compute the predictions for the next `horizon` steps using `predict_fn`.\"\"\"\n",
    "    model_future = self.client.scatter(self.model.model_, broadcast=True)\n",
    "    predictions_futures = self.client.map(predict_fn,\n",
    "                                          self.ts,\n",
    "                                          model=model_future,\n",
    "                                          horizon=horizon,\n",
    "                                          **predict_fn_kwargs)\n",
    "    meta = self.client.submit(lambda x: x.head(), predictions_futures[0]).result()\n",
    "    return dd.from_delayed(predictions_futures, meta=meta, divisions=self.data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b21c7-23fa-4628-be0d-4da2448fa382",
   "metadata": {},
   "source": [
    "The `DistributedForecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing predictions) and applies them in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009c1fe-caba-4412-be41-69588a512bbf",
   "metadata": {},
   "source": [
    "## Example\n",
    "This shows an example with simulated data, for a real world example in a remote cluster you can check the [M5 distributed example](https://www.kaggle.com/lemuz90/m5-mlforecast-distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc621d-43b8-4160-a694-08d3259e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from window_ops.expanding import expanding_mean\n",
    "from window_ops.rolling import rolling_mean, rolling_std\n",
    "\n",
    "from mlforecast.distributed.models.lgb import LGBMForecast\n",
    "from mlforecast.distributed.models.xgb import XGBForecast\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9b623-af02-4acb-9d09-cda1debead4e",
   "metadata": {},
   "source": [
    "The different things that you need to use `DistributedForecast` (as opposed to `Forecast`) are:\n",
    "1. You need to set up a `dask.distributed.Client`. If this client is connected to a remote cluster then the process will run there.\n",
    "2. Your data needs to be a `dask.dataframe`.\n",
    "3. You need to use a model that implements distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479a531-60db-4c1e-ac28-189b8628d3ef",
   "metadata": {},
   "source": [
    "### Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8d3ef-d1f3-48da-b2dc-0579ea058626",
   "metadata": {},
   "source": [
    "Here we define a client that connects to a `dask.distributed.LocalCluster`, however it could be any other kind of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea1b0f-f8d3-4be5-89fe-3dcbe00dff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cfccf-5898-4046-a6c8-091fec26f0e4",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "The data is given as a `dask.dataframe`, you need to make sure that each time serie is only in one partition and it is recommended that you have as many partitions as you have workers.\n",
    "\n",
    "The required input format is the same as for `Forecast`, except that it's a `dask.dataframe` instead of a `pandas.dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388b1cf-78cb-4cf9-b146-6d2043d4d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2, equal_ends=True)\n",
    "partitioned_series = dd.from_pandas(series, npartitions=2)\n",
    "partitioned_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92d81-3a70-424d-81dc-8a995af1fb9c",
   "metadata": {},
   "source": [
    "### Model\n",
    "In order to perform distributed forecasting, we need to use a model that is able to train in a distributed way using `dask`. The current implementations are in `LGBMForecast` and `XGBForecast` which are just wrappers around `lightgbm.dask.DaskLGBMRegressor` and `xgboost.dask.DaskXGBRegressor` that add a `model_` property to get the trained model from them and send it to every worker to perform the predictions step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8012f8-3325-41a2-8fb0-7eaa07a24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMForecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6281f-1e65-433b-aad2-2d9895a64bcb",
   "metadata": {},
   "source": [
    "### Flow configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce63471-cd49-451f-9ce9-f7a484c61549",
   "metadata": {},
   "source": [
    "As in the local version (`Forecast`) a flow configuration is required. This is passed as a dictionary that will be unpacked in the call to our preprocessing function, which is`preprocessing_flow` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9ce1f-b0c2-4e81-8f78-f4781fa536d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_flow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a14f0-4103-450d-8540-5d825044bf07",
   "metadata": {},
   "source": [
    "Here where we say that:\n",
    "1. Our series have daily frequency.\n",
    "2. We want to use lag 7 and lag 14 as features.\n",
    "3. We want the lag transformations to be:\n",
    "   * expanding mean of the lag 1\n",
    "   * rolling mean of the lag 7 over a window of size 7\n",
    "   * rolling standard deviation of the lag 7 over a window of size 7\n",
    "4. We want to use dayofweek, month and year as date features.\n",
    "5. We want to perform the preprocessing and the forecasting steps using 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b682a9-7b7a-429f-8d25-a0d0ae4aafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        1: [expanding_mean],\n",
    "        7: [\n",
    "            (rolling_mean, 7), \n",
    "            (rolling_std, 7),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    num_threads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e1983-7235-4930-9b44-36c76e35be69",
   "metadata": {},
   "source": [
    "### Training\n",
    "Once we have our model and flow configuration we instantiate a `DistributedForecast` with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d20c9-5ad7-42e8-a6b2-40d8005467bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(model, flow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65875de6-71f3-4dda-bcd5-388f943cecfa",
   "metadata": {},
   "source": [
    "And we fit it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472ae1e-4b26-4414-8c58-9cc9a9e4d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(partitioned_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bae3f-00fc-44be-91c6-005bd052eaf7",
   "metadata": {},
   "source": [
    "This computes the features for each partition independently and performs distributed training. \n",
    "\n",
    "If you only want to compute the features (without fitting the model) you can use `DistributedForecast.preprocess` instead, which will return the dask dataframe with the computed features. Note that if you then want to train the model you have to call `DistributedForecast.model.fit` instead of `Forecast.fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961fb48-54de-48f1-a868-824bed51f596",
   "metadata": {},
   "source": [
    "### Forecasting\n",
    "Once we have our fitted model we can compute the predictions for the next 7 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a439d-e0d0-4f2f-925c-fccd92361c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fcst.predict(7)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33846e-6d3c-4199-afe8-c5e064a53aac",
   "metadata": {},
   "source": [
    "### Backtesting\n",
    "\n",
    "If we would like to know how good our forecast will be for a specific model and set of features then we can perform backtesting. What backtesting does is take our data and split it in two parts, where the first part is used for training and the second one for validation. Since the data is time dependant we usually take the last *x* observations from our data as the validation set.\n",
    "\n",
    "This process is implemented in `Forecast.backtest` (and inherited by `DistributedForecast`), which takes our data and performs the process described above for `n_windows` times where each window is of size `window_size`. For example, if we have 100 samples and we want to perform 2 backtests each of size 14, the splits will be as follows:\n",
    "\n",
    "1. Train: 1 to 72. Validation: 73 to 86.\n",
    "2. Train: 1 to 86. Validation: 87 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab05e3-3d72-4afa-a74e-6961a3d9186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 2\n",
    "window_size = 14\n",
    "\n",
    "fcst = DistributedForecast(model, flow_config)\n",
    "backtest_results = fcst.backtest(partitioned_series, n_windows, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff391af-3c38-45b1-8e16-34da01acac77",
   "metadata": {},
   "source": [
    "This returns a generator that yields the results of each window one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fded328-f113-48dc-80e6-913677eb692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window1_result = next(backtest_results)\n",
    "window1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1698f-6f21-4389-b00c-83713ce36935",
   "metadata": {},
   "outputs": [],
   "source": [
    "window2_result = next(backtest_results)\n",
    "results = pd.concat([window1_result.compute(), window2_result.compute()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffbd79-f544-4cfc-8b62-ba009165c071",
   "metadata": {},
   "source": [
    "We can aggregate these by date to get a rough estimate of how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4a701-1779-4007-b9be-dbdee3454bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = results.groupby('ds').sum()\n",
    "agg_results.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8651d-bf63-4a42-bd19-95e1271fed95",
   "metadata": {},
   "source": [
    "We can include some more context by using the values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd06319-4031-422e-bd1e-93c56ca2bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = series[series.ds < agg_results.index.min()]\n",
    "agg_history = history.groupby('ds')[['y']].sum().tail(50)\n",
    "agg_history.append(agg_results).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da220459-5595-4bd4-901f-4c4f5ee8090c",
   "metadata": {},
   "source": [
    "Note that since the backtest results are returned as a generator we can also compute a single statistic on them and not keep the whole results in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c787d-e68b-4a46-bf1b-8c865dc5cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_from_dask_dataframe(ddf):\n",
    "    ddf['sq_err'] = (ddf['y'] - ddf['y_pred'])**2\n",
    "    mse = ddf['sq_err'].mean()\n",
    "    return mse.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5d0e9-531f-45ad-aadb-250db1accf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = DistributedForecast(LGBMForecast(), flow_config)\n",
    "backtest_results = fcst.backtest(partitioned_series, n_windows, window_size)\n",
    "\n",
    "losses = [mse_from_dask_dataframe(res) for res in backtest_results]\n",
    "np.round(losses, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4c3dd-2d97-4052-94da-e941105725e5",
   "metadata": {},
   "source": [
    "We can do the same for `XGBForecast`, however we have to label encode our categories first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5361b5-3cc3-4c6f-9d1d-ce418e163738",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_series = partitioned_series.copy()\n",
    "for col in encoded_series.select_dtypes(include='category'):\n",
    "    encoded_series[col] = encoded_series[col].cat.codes\n",
    "\n",
    "fcst = DistributedForecast(XGBForecast(), flow_config)\n",
    "backtest_results = fcst.backtest(encoded_series, n_windows, window_size)\n",
    "losses = [mse_from_dask_dataframe(res) for res in backtest_results]\n",
    "np.round(losses, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
