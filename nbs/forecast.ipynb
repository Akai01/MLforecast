{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa33fa-816f-463f-9215-9559b0ddd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20376798-3d26-4c74-9e52-d5b657b7768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccedaf1-56c9-4aaf-af7b-fe049df299ad",
   "metadata": {},
   "source": [
    "# Forecast\n",
    "\n",
    "> Full pipeline encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b089a52-e06d-49b1-9328-793cffe56045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from mlforecast.core import predictions_flow, preprocessing_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528852a6-4e5d-4160-b398-92904c456a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from window_ops.rolling import *\n",
    "from window_ops.expanding import *\n",
    "from window_ops.ewm import ewm_mean\n",
    "\n",
    "from mlforecast.utils import generate_daily_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a9f0c-02d3-4d07-8e21-2134c3afe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Forecast:\n",
    "    \"\"\"Full pipeline encapsulation. \n",
    "    \n",
    "    Takes a model (scikit-learn compatible regressor) and a flow configuration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, flow_config: Dict):\n",
    "        self.model = model\n",
    "        self.flow_config = flow_config\n",
    "        \n",
    "    def preprocess(self, data: pd.DataFrame, prep_fn: Callable = preprocessing_flow) -> pd.DataFrame:\n",
    "        \"\"\"Apply the transformations defined in the flow configuration.\"\"\"\n",
    "        self.ts, series_df = prep_fn(data, **self.flow_config)\n",
    "        return series_df\n",
    "    \n",
    "    def fit(self, data: pd.DataFrame, prep_fn: Callable = preprocessing_flow, **kwargs) -> 'Forecast':\n",
    "        \"\"\"Perform the preprocessing and fit the model.\"\"\"\n",
    "        series_df = self.preprocess(data, prep_fn)\n",
    "        X, y = series_df.drop(columns=['ds', 'y']), series_df.y.values\n",
    "        del series_df\n",
    "        self.model.fit(X, y, **kwargs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, horizon: int, predict_fn: Callable = predictions_flow) -> pd.DataFrame:\n",
    "        \"\"\"Compute the predictions for the next `horizon` steps.\"\"\"\n",
    "        return predict_fn(self.ts, self.model, horizon)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Forecast(model={self.model}, flow_config={self.flow_config})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a3564-8ae3-47fc-80d9-af49d6661844",
   "metadata": {},
   "source": [
    "The `Forecast` class is a high level abstraction that encapsulates all the steps in the pipeline (preprocessing, fitting the model and computing the predictions). It tries to mimic the scikit-learn API.\n",
    "\n",
    "In order to perform forecasting for some time series you need a dataframe with `unique_id` as the index (which contains the identifier for each time serie), a `ds` column with the datestamps and a `y` column with the series values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c776bb-b87b-4c01-a67a-ff95a055b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_daily_series(100, n_static_features=2)\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef3887-8163-4c0f-b12d-3f28902b6a2f",
   "metadata": {},
   "source": [
    "Whatever extra columns you have, like `static_0` and `static_1` here are considered to be static and are replicated when constructing the features for the next datestamp. You can disable this by passing `static_features` to the flow configuration, which will only keep the columns you define there as static. Keep in mind that they will still be used for training, so you'll have to write your own function to populate the non-static columns and pass it to the `Forecast.predict` method. This is shown in the M5 example.\n",
    "\n",
    "The next step is definining the flow configuration, where we say that:\n",
    "1. Our series have daily frequency.\n",
    "2. We want to use lag 7 and lag 14 as features.\n",
    "3. We want to use the expanding mean of the lag 1 as well as the rolling mean and the rolling standard deviation of the lag 7 over a window of size 7) as features.\n",
    "4. We want to use dayofweek, month and year as date features.\n",
    "5. We want to perform the preprocessing and the forecasting using 2 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfddfe2-7858-42b5-a10b-94a54ca0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_config = dict(\n",
    "    freq='D',\n",
    "    lags=[7, 14],\n",
    "    lag_transforms={\n",
    "        1: [\n",
    "            expanding_mean\n",
    "        ],\n",
    "        7: [\n",
    "            (rolling_mean, 7), \n",
    "            (rolling_std, 7),\n",
    "        ]\n",
    "    },\n",
    "    date_features=['dayofweek', 'month', 'year'],\n",
    "    num_threads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854f615-a7a1-45d9-9122-852478db2f8d",
   "metadata": {},
   "source": [
    "Once we have this configuration we just instantiate a `Forecast` object with the model we want to use and this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62a31f-b470-49a8-bb05-73b3e6c02fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = Forecast(lgb.LGBMRegressor(), flow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd38e85-4369-4711-b104-fd2ee4e6e8fb",
   "metadata": {},
   "source": [
    "And we fit it to our series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6a43b-8bcf-4ea4-95fb-0f1460521f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab94427-65bc-42f6-9cb5-ac9d5240758a",
   "metadata": {},
   "source": [
    "Once we have this fitted model, we can compute the forecasts for the next 7 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b224be-4f15-4b2b-8985-d8a590a384b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.predict(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe74b7-f0a3-4105-a11e-6da5b09ab7fe",
   "metadata": {},
   "source": [
    "This uses each prediction as the next value of the target and updates all features accordingly. The static features were propagated and the date features were computed using each new datestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8497a0-3608-446f-a71d-b921ebfcd3e8",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7baab-1083-4565-8c90-cbcc72d8a3be",
   "metadata": {},
   "source": [
    "If we want to validate how our model performs using this rolling predictions scheme, we can first split our data removing the last observations from each serie and keeping it as a validation set, compute the forecast and then compare these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938d72b-422b-4ca2-9f56-5f6f090832da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_n_mask(serie, n):\n",
    "    mask = np.full_like(serie, False, dtype=bool)\n",
    "    mask[-n:] = True\n",
    "    return mask\n",
    "\n",
    "test_size = 14\n",
    "valid_mask = series.groupby('unique_id')['y'].transform(get_last_n_mask, test_size)\n",
    "train = series[~valid_mask]\n",
    "y_valid = series[valid_mask].set_index('ds', append=True)[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23228b2-a738-4afc-89e8-26320681b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preds(train, model, cats2int=False):\n",
    "    if cats2int:\n",
    "        train = train.copy()\n",
    "        for col in train.select_dtypes(include='category'):\n",
    "            train[col] = train[col].cat.codes\n",
    "            \n",
    "    fcst = Forecast(model, flow_config)\n",
    "    fcst.fit(train)\n",
    "    preds = fcst.predict(test_size)\n",
    "\n",
    "    evals = y_valid.join(preds.set_index('ds', append=True))\n",
    "    evals['sq_err'] = (evals['y'] - evals['y_pred'])**2\n",
    "    mse = evals.groupby('unique_id')['sq_err'].mean().mean()\n",
    "    print(f'MSE: {mse:.1f}')\n",
    "    \n",
    "    valid_sum = y_valid.groupby('ds').sum()\n",
    "    preds_sum = preds.groupby('ds')['y_pred'].sum()\n",
    "    valid_sum.join(preds_sum).plot(marker='.', figsize=(16, 6));    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff8ed3-bae0-4607-9e16-85f710f74345",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds(train, lgb.LGBMRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f299bf-e911-4d27-854f-ab5ff6104129",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds(train, xgb.XGBRegressor(), cats2int=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
